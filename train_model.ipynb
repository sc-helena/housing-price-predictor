{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from config import DATASET\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(DATASET) # saved in cache\n",
    "\n",
    "print(\"Path to dataset files:\", path) \n",
    "\n",
    "def load_housing_data():\n",
    "    # loaded to pandas\n",
    "    return pd.read_csv(os.path.join(path, \"housing.csv\")) \n",
    "\n",
    "housing = load_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_corr(data):\n",
    "    # Select only the numeric columns\n",
    "    numeric_data = data.select_dtypes(include=[float, int])\n",
    "\n",
    "    # Calculate the correlation matrix on the numeric data\n",
    "    corr_matrix = numeric_data.corr()\n",
    "    corr_matrix[\"median_house_value\"].sort_values(ascending=False)\n",
    "    # Display the correlation matrix\n",
    "    print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"rooms_per_house\"] = housing[\"total_rooms\"] / housing[\"households\"]\n",
    "housing[\"bedrooms_ratio\"] = housing[\"total_bedrooms\"] / housing[\"total_rooms\"]\n",
    "housing[\"people_per_house\"] = housing[\"population\"] / housing[\"households\"]\n",
    "\n",
    "print_corr(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.figures import save_fig\n",
    "\n",
    "housing.reset_index()\n",
    "num_pipeline = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")), # insert missing values\n",
    "    (\"scaler\", MinMaxScaler(feature_range=(-1, 1))), # scale num values\n",
    "])\n",
    "\n",
    "housing_num = housing.select_dtypes(include=[np.number]) # numeric values\n",
    "housing_num_tarnsformed_arrays = num_pipeline.fit_transform(housing_num)\n",
    "housing_num_transformed_df = pd.DataFrame(housing_num_tarnsformed_arrays, columns=housing_num.columns,\n",
    "                          index=housing_num.index)\n",
    "\n",
    "\n",
    "# plot changes\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "\n",
    "housing_num_transformed_df.hist(bins=50, figsize=(12, 8))\n",
    "save_fig(\"attribute_histogram_plots_after_scaling\")  # extra code\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat = housing[[\"ocean_proximity\"]] # categorial attribute\n",
    "housing_cat.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# One Hot encoder \n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "# Convert to a NumPy array\n",
    "housing_cat_1hot_dense = housing_cat_1hot.toarray()\n",
    "# Get the feature names (categories) from the encoder\n",
    "categories = cat_encoder.get_feature_names_out(input_features=housing_cat.columns)\n",
    "# Convert to a pandas DataFrame\n",
    "housing_cat_1hot_df = pd.DataFrame(housing_cat_1hot_dense, columns=categories, index=housing_cat.index)\n",
    "housing_transformed = pd.concat([housing_num_transformed_df, housing_cat_1hot_df], axis=1)\n",
    "print(housing_transformed[\"median_income\"].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed to ensure same output for multiple runs\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "random_seed = 3\n",
    "\n",
    "# ensure instances from each median income stratum \n",
    "housing_transformed[\"income_cat\"] = pd.cut(housing_transformed[\"median_income\"],\n",
    "                               bins=[-1.1, -0.6, -0.2, 0.2, 0.6, 1.1],\n",
    "                               labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "housing_transformed[\"income_cat\"].value_counts().sort_index().plot.bar(rot=0, grid=True)\n",
    "plt.xlabel(\"Income category\")\n",
    "plt.ylabel(\"Number of districts\")\n",
    "plt.show()\n",
    "\n",
    "# use the stratisfied splitter\n",
    "strat_train_set, strat_test_set = train_test_split(\n",
    "    housing_transformed, test_size=0.2, stratify=housing_transformed[\"income_cat\"], random_state=42)\n",
    "\n",
    "# income_cat not needed anymore\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pickle import dump\n",
    "from config import MODEL_PATH\n",
    "\n",
    "housing_labels_train = strat_train_set[\"median_house_value\"]\n",
    "model = TransformedTargetRegressor(LinearRegression(),\n",
    "                                   transformer=StandardScaler())\n",
    "model.fit(strat_train_set[[\"median_income\"]], housing_labels_train)\n",
    "\n",
    "model_file_path = os.path.join(MODEL_PATH, \"price_estimator_model.pkl\")\n",
    "\n",
    "with open(model_file_path, \"wb\") as file:\n",
    "    dump(model, file, protocol=5) # set protocol 5 to reduce memory usage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
